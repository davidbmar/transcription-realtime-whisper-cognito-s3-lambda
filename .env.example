# AWS Configuration
AWS_REGION=us-east-2
AWS_ACCOUNT_ID=your-account-id

# Serverless Service Name (used for resource naming)
SERVICE_NAME=clouddrive-app

# Cognito Configuration (get these from your CloudDrive deployment)
COGNITO_S3_BUCKET=your-s3-bucket-name
COGNITO_DOMAIN=your-cognito-domain
COGNITO_USER_POOL_ID=us-east-2_XXXXXXXXX
COGNITO_USER_POOL_CLIENT_ID=xxxxxxxxxxxxxxxxxx
COGNITO_IDENTITY_POOL_ID=us-east-2:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
COGNITO_API_ENDPOINT=https://xxxxxxxxxx.execute-api.us-east-2.amazonaws.com/dev
COGNITO_CLOUDFRONT_URL=https://xxxxxxxxxxxxx.cloudfront.net

# CloudDrive Browser Testing Credentials
# NOTE: These will be auto-prompted on first run if not set
# The browser-test.js script will save them here automatically
# NEVER commit .env to git - it's in .gitignore
CLOUDDRIVE_TEST_EMAIL=your-email@example.com
CLOUDDRIVE_TEST_PASSWORD=your-password

# Optional: Run browser in headed mode (visible)
# HEADLESS=false

# WhisperLive Configuration (for audio transcription)
# IMPORTANT: Use EDGE BOX architecture (secure, recommended by scripts 030/031)
#
# NEW ARCHITECTURE: Dynamic IP Lookup (no static IPs!)
# - GPU_INSTANCE_ID is the permanent EC2 instance ID (never changes)
# - Public IPs change on every stop/start (DO NOT store in .env)
# - All scripts use get_instance_ip() for runtime IP lookup
# - Edge box Caddy proxy auto-updates with script 825
#
# Architecture: Client → wss://EDGE_BOX/ws → ws://GPU:9090
#
# Required Configuration:
GPU_INSTANCE_ID=i-xxxxxxxxxxxxx     # GPU EC2 instance ID (permanent)
WHISPERLIVE_PORT=9090               # GPU WhisperLive port
#
# Edge Box Configuration (for browser clients):
WHISPERLIVE_WS_URL=wss://your-edge-box-public-ip/ws  # Clients connect here
EDGE_BOX_DNS=your-edge-box-public-ip                 # Edge box IP/DNS
EDGE_BOX_INSTANCE_ID=i-xxxxxxxxxxxxx                 # Edge box instance ID (if EC2)
#
# DEPRECATED (no longer used - IPs looked up dynamically):
# GPU_INSTANCE_IP=<do-not-set>    # Looked up from GPU_INSTANCE_ID
# GPU_HOST=<do-not-set>           # Looked up from GPU_INSTANCE_ID
# WHISPERLIVE_HOST=<do-not-set>   # Looked up from GPU_INSTANCE_ID
# RIVA_HOST=<do-not-set>          # Looked up from GPU_INSTANCE_ID

# Batch Transcription Configuration
# Used by scripts/515-run-batch-transcribe.sh for optimized parallel processing
#
# Performance Tuning:
# - BATCH_SIZE: Number of chunks to process in a single batch (default: 100)
#   * Higher = fewer model reloads, faster total time
#   * Lower = more frequent progress updates, easier to debug
#   * Recommended: 100 for production, 10 for testing
#
# - BATCH_MAX_PARALLEL_*: Concurrent S3 operations (default: 20)
#   * Higher = faster downloads/uploads, more system resources
#   * Lower = more conservative, safer for resource-constrained systems
#   * AWS S3 limit: ~5,500 requests/second per prefix
#   * Recommended: 20-40 depending on available memory/network
#
# - BATCH_DOWNLOAD_THRESHOLD: When to start GPU processing (default: 30)
#   * Batches <30: Wait for all downloads (minimize transfer overhead)
#   * Batches ≥30: Start GPU after 30 files (faster startup)
#
# - WHISPER_MODEL: Accuracy vs speed trade-off
#   * tiny.en (39M params): 4-5x faster, lower accuracy
#   * base.en (74M params): 2-3x faster, good accuracy [RECOMMENDED]
#   * small.en (244M params): Baseline speed, better accuracy [DEFAULT]
#   * medium.en (769M params): 2x slower, best accuracy
#
# - WHISPER_COMPUTE_TYPE: Precision vs speed
#   * int8: ~2x faster, minimal accuracy loss [RECOMMENDED FOR SPEED]
#   * int8_float16: 1.5x faster, hybrid approach
#   * float16: Baseline speed [DEFAULT]
#   * float32: Slower, no benefit
#
# Example configurations:
#   Fastest (2-3x speedup): WHISPER_MODEL=base.en, WHISPER_COMPUTE_TYPE=int8
#   Balanced: WHISPER_MODEL=small.en, WHISPER_COMPUTE_TYPE=int8 [DEFAULT]
#   Best Quality: WHISPER_MODEL=medium.en, WHISPER_COMPUTE_TYPE=int8
#
BATCH_SIZE=100
BATCH_MAX_PARALLEL_DOWNLOAD=20
BATCH_MAX_PARALLEL_UPLOAD=20
BATCH_DOWNLOAD_THRESHOLD=30
BATCH_DOWNLOAD_TIMEOUT=60
WHISPER_MODEL=small.en
WHISPER_COMPUTE_TYPE=int8

# GPU Configuration for Batch Processing
# NOTE: GPU_INSTANCE_ID is defined above in WhisperLive section
GPU_SSH_KEY_PATH=/home/ubuntu/.ssh/your-key.pem
GPU_HOURLY_COST=0.526                      # g4dn.xlarge on-demand pricing

# Smart Batch Scheduler Configuration
# Used by scripts/535-smart-batch-scheduler.sh for automated GPU management
#
# Threshold-based scheduling:
# - BATCH_THRESHOLD: Min chunks to trigger GPU startup (default: 100)
#   * Higher = fewer GPU startups, more efficient per session
#   * Lower = more responsive, shorter queue wait times
#   * Recommended: 100 for production, 50 for testing
#
# - BATCH_SCHEDULER_CHECK_HOURS: How often to check queue (default: 2)
#   * Used by cron/systemd timer
#   * More frequent = more responsive, more API calls
#   * Less frequent = more efficient, longer wait times
#
# - BATCH_MAX_RUNTIME_HOURS: Safety cutoff (default: 2)
#   * Prevents runaway processes
#   * Should be > expected max batch duration
#
BATCH_THRESHOLD=100
BATCH_SCHEDULER_CHECK_HOURS=2
BATCH_MAX_RUNTIME_HOURS=2

# Google Docs Integration (optional)
# Used by .claude/skills/google-docs-edit/ and scripts/505-*, 510-*
GOOGLE_DOC_ID=your-google-doc-id-here
GOOGLE_CREDENTIALS_PATH=/path/to/credentials.json  # Defaults to google-docs-test/credentials.json
GOOGLE_CREDENTIALS_BASE64=  # Base64-encoded credentials.json for Lambda (auto-generated by script 510)

# Batch Transcription Verification & Retry Settings (optional)
# Used by scripts/515-run-batch-transcribe.sh
#
# - MAX_RETRY_ATTEMPTS: Maximum retry attempts for failed chunks (default: 3)
#   * How many times to retry transcription if chunks are still missing
#   * Each retry runs full scanner + transcription cycle
#   * Higher = more resilient, but longer runtime on persistent failures
#
# - EDGE_LOG_FILE: Centralized log file for batch events (default: /var/log/batch-transcription.log)
#   * Records all batch start/stop/retry/verification events
#   * Format: timestamp|epoch|event|details
#   * Useful for monitoring and debugging batch processing
#
MAX_RETRY_ATTEMPTS=3
EDGE_LOG_FILE=/var/log/batch-transcription.log

# AI-Powered Transcript Analysis (optional)
# Used by scripts/520-generate-ai-analysis.sh for intelligent video navigation
#
# This feature analyzes transcripts with Claude AI to extract:
# - Action Items: Tasks and recommendations
# - Key Terms: Technical vocabulary and important concepts
# - Key Themes: Main topics discussed
# - Topic Changes: Where speaker shifts subjects
# - Overall Highlights: Most significant moments
#
# To enable AI analysis:
# 1. Get API key from: https://console.anthropic.com/settings/keys
# 2. Add it below (NEVER commit .env to git!)
# 3. Run: ./scripts/520-generate-ai-analysis.sh --session-path <path>
#
# Cost: ~$0.002 per transcript (using Claude Haiku)
# Model: claude-haiku-4 (fast, cost-effective for structured extraction)
#
ANTHROPIC_API_KEY=
