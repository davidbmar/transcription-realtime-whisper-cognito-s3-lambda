<!DOCTYPE html>
<html>
<head>
    <title>WhisperLive Test Client</title>
    <style>
        body { font-family: Arial; padding: 20px; max-width: 800px; margin: 0 auto; }
        button { padding: 10px 20px; margin: 5px; font-size: 16px; }
        #status { padding: 10px; margin: 10px 0; border-radius: 5px; }
        #transcript { min-height: 200px; padding: 10px; border: 1px solid #ccc; margin: 10px 0; white-space: pre-wrap; }
        .connected { background: #d4edda; }
        .disconnected { background: #f8d7da; }
        .recording { background: #d1ecf1; }
    </style>
</head>
<body>
    <h1>WhisperLive Test Client</h1>
    <div id="status" class="disconnected">Disconnected</div>
    <button id="start">Start Recording</button>
    <button id="stop" disabled>Stop Recording</button>
    <h3>Transcript:</h3>
    <div id="transcript"></div>

    <script>
        let ws = null;
        let audioContext = null;
        let mediaStream = null;
        let processor = null;
        let isRecording = false;

        const startBtn = document.getElementById('start');
        const stopBtn = document.getElementById('stop');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');

        function setStatus(text, className) {
            statusDiv.textContent = text;
            statusDiv.className = className;
        }

        function addTranscript(text) {
            transcriptDiv.textContent += text + '\n';
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        // NO CONVERSION NEEDED! WhisperLive expects Float32Array directly
        // ChatGPT was right - most WhisperLive examples use Float32, not Int16

        async function startRecording() {
            try {
                setStatus('Connecting...', 'connecting');

                // Connect WebSocket
                const wsUrl = `wss://${window.location.host}/ws`;
                ws = new WebSocket(wsUrl);
                ws.binaryType = 'arraybuffer';

                ws.onopen = async () => {
                    console.log('WebSocket connected');

                    // Send config
                    const config = {
                        uid: 'browser-' + Date.now(),
                        task: 'transcribe',
                        language: 'en',
                        model: 'Systran/faster-whisper-small.en',
                        use_vad: false
                    };
                    console.log('Sending config:', config);
                    ws.send(JSON.stringify(config));

                    // Start audio capture
                    mediaStream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            channelCount: 1,
                            sampleRate: 16000,
                            echoCancellation: true,
                            noiseSuppression: true
                        }
                    });

                    audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                    const source = audioContext.createMediaStreamSource(mediaStream);

                    // Create ScriptProcessor for raw audio
                    processor = audioContext.createScriptProcessor(4096, 1, 1);

                    processor.onaudioprocess = (e) => {
                        if (!isRecording || ws.readyState !== WebSocket.OPEN) return;

                        // Send Float32Array directly - WhisperLive expects Float32!
                        const audioData = e.inputBuffer.getChannelData(0);
                        ws.send(audioData.buffer);
                    };

                    source.connect(processor);
                    processor.connect(audioContext.destination);

                    isRecording = true;
                    setStatus('Recording... Speak now!', 'recording');
                    startBtn.disabled = true;
                    stopBtn.disabled = false;
                };

                ws.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        console.log('Received:', data);

                        if (data.segments && data.segments.length > 0) {
                            // Concatenate all segment texts into one line
                            const fullText = data.segments.map(s => s.text || '').join('');

                            // Update transcript in place (clear and replace)
                            if (fullText.trim()) {
                                transcriptDiv.textContent = fullText;
                                transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
                            }
                        } else if (data.message === 'SERVER_READY') {
                            console.log('WhisperLive ready');
                        }
                    } catch (e) {
                        console.log('Non-JSON message:', event.data);
                    }
                };

                ws.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    setStatus('Connection error', 'disconnected');
                };

                ws.onclose = (event) => {
                    console.log('WebSocket closed:', event.code, event.reason);
                    setStatus(`Disconnected (${event.code})`, 'disconnected');
                    stopRecording();
                };

            } catch (error) {
                console.error('Failed to start:', error);
                setStatus('Error: ' + error.message, 'disconnected');
            }
        }

        function stopRecording() {
            isRecording = false;

            if (processor) {
                processor.disconnect();
                processor = null;
            }

            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }

            if (ws) {
                ws.close();
                ws = null;
            }

            setStatus('Disconnected', 'disconnected');
            startBtn.disabled = false;
            stopBtn.disabled = true;
        }

        startBtn.onclick = startRecording;
        stopBtn.onclick = stopRecording;
    </script>
</body>
</html>
